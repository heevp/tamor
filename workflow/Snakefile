#!/usr/bin/env -S snakemake --cores 1 --quiet --cluster sbatch --use_conda --snakefile

import os.path
import tempfile
import csv
from pathlib import Path
import glob

from tamor_utils import decomment

# Site-specific config, most changes would be made in that file rather than this one.
configfile: "config/config.yaml"

analysis_dir = config["analysis_dir"]
bcl_dir = config["bcl_dir"]
output_dir = config["output_dir"]
samplesheet_dir = config["samplesheet_dir"]
temp_dir = config["temp_dir"]
sequencer = config["sequencer"]
refgenome = config["refgenome"]
ref_anchored_genome = config["ref_anchored_genome"]
ref_fasta = config["ref_fasta"]
ref_exon_annotations = config["ref_exon_annotations"]
rna_quantification_library_type = config["rna_quantification_library_type"]
tumor_in_normal_tolerance_proportion = config["tumor_in_normal_tolerance_proportion"]
ora_reference = config["ora_reference"]

samplesheet_cache = {}

# The format of the paired DNA samples file is tab-delimited, with subject ID in the zeroth column, tumor sample name in first column, then matched normal in the second. 
# Additional columns are metadata: Boolean indicating if some tumor is expected in the normal sample, and a number representing the tumor site as per PCGR guidelines (see README.md for details), then finally project name in the last column.
dna_paired_samples = {}
with open(config["dna_paired_samples_tsv"], 'r') as data_in:
	tsv_file = csv.reader(decomment(data_in), delimiter="\t")
	for line in tsv_file:
		if len(line[0]) > 35 or len(line[0]) < 6:
			raise NameError("Subject names are limited to between 6 and 35 characters by PCGR reporting requirements, please revise "+line[0])
		if '_' in line[0]:
			raise NameError("Subject name cannot contain any underscores ('_'), please revise "+line[0])
		if '_' in line[1]:
			raise NameError("Tumor sample name cannot contain any underscores ('_'), please revise "+line[1])
		if '_' in line[2]:
			raise NameError("Germline sample name cannot contain any underscores ('_'), please revise "+line[2])
		dna_paired_samples_key = "_".join((line[0],line[1],line[2]))
		dna_paired_samples[dna_paired_samples_key] = [line[0],line[3],line[4],line[5]]

# Fourth column is the project, and fifth column is the source tissue type abbreviation per https://omnideconv.org/immunedeconv/articles/immunedeconv.html.
rna_paired_samples = {} 
with open(config["rna_paired_samples_tsv"], 'r') as data_in:
	tsv_file = csv.reader(decomment(data_in), delimiter="\t")
	for line in tsv_file:
		rna_paired_samples_key = "_".join((line[0],line[1]))
		rna_paired_samples[rna_paired_samples_key] = [line[0],line[1],line[3]]

def get_dna_projects(dna_paired_sample_values):
        return [tuple[3] for tuple in dna_paired_sample_values]

def get_rna_projects(rna_sample_values):
        return [tuple[2] for tuple in rna_sample_values]

def get_subjects(paired_sample_values):
	return [tuple[0] for tuple in paired_sample_values]

# The format of the paired RNA samples file is tab-delimited, with subject ID in the zeroth column, tumor RNA sample name in first column, then matched normal *DNA* in the second.

rule all:
	# Generate a Dragen somatic mutation analysis for each pair for tumor-normal listed in the DNA pairs file.
	# Generate RNA quantification for samples listed in the RNA samples file.
	# Also generate a PCGR user-friendly variant report, which will require the germline variant calls as well in another rule.
	input:
		expand(output_dir+"/{project_name}/{subject}/{sample_combo}.dna.somatic.hard-filtered.vcf.gz", zip, project_name=get_dna_projects(dna_paired_samples.values()), subject=get_subjects(dna_paired_samples.values()), sample_combo=list(dna_paired_samples.keys())),
		expand(output_dir+"/{project_name}/{subject}/{sample_combo}.dna.somatic.cnv.vcf.gz", zip, project_name=get_dna_projects(dna_paired_samples.values()), subject=get_subjects(dna_paired_samples.values()), sample_combo=list(dna_paired_samples.keys())),
		expand(output_dir+"/{project_name}/{subject}/{sample_combo}.dna.somatic.sv.vcf.gz", zip, project_name=get_dna_projects(dna_paired_samples.values()), subject=get_subjects(dna_paired_samples.values()), sample_combo=list(dna_paired_samples.keys())),
		expand(output_dir+"/{project_name}/{subject}/{sample_combo}.dna.somatic.hla.tsv", zip, project_name=get_dna_projects(dna_paired_samples.values()), subject=get_subjects(dna_paired_samples.values()), sample_combo=list(dna_paired_samples.keys())),
		expand(output_dir+"/pcgr/{project_name}/{sample_combo}/{subject}.pcgr.grch38.html", zip, project_name=get_dna_projects(dna_paired_samples.values()), subject=get_subjects(dna_paired_samples.values()), sample_combo=list(dna_paired_samples.keys())),
		#expand(output_dir+"/{project_name}/{subject}/{sample_combo}.dna.somatic.hard-filtered.maf.txt", zip, project_name=get_dna_projects(dna_paired_samples.values()), subject=get_subjects(dna_paired_samples.values()), sample_combo=list(dna_paired_samples.keys())),

		expand(output_dir+"/{project_name}/{subject}/rna/{sample_combo}.rna.quant.genes.hugo.tpm.txt", zip, project_name=get_rna_projects(rna_paired_samples.values()), subject=get_subjects(rna_paired_samples.values()), sample_combo=list(rna_paired_samples.keys())),
		expand(output_dir+"/{project_name}/{subject}/rna/{sample_combo}.rna.fusion_candidates.features.csv", zip, project_name=get_rna_projects(rna_paired_samples.values()), subject=get_subjects(rna_paired_samples.values()), sample_combo=list(rna_paired_samples.keys()))

def get_samplesheet(wildcards):
	return samplesheet_dir+'/'+wildcards.run+'.csv'

def get_tumor_site(wildcards):
	return dna_paired_samples["_".join((wildcards.subject, wildcards.tumor, wildcards.normal))][2]

def get_normal_contains_some_tumor(wildcards):
	return dna_paired_samples["_".join((wildcards.subject, wildcards.tumor, wildcards.normal))][1]

def load_samplesheet_info():
	if samplesheet_cache:
		return samplesheet_cache
	for path in Path(samplesheet_dir).iterdir():
		if path.is_file() and Path(path).suffix == '.csv':
			#print("Reading file " + str(path))
			samplesheet_lines = []
			current_file = open(path, "r")
			csv_file = csv.reader(current_file)
			sample_name_index = -1
			sample_id_index = -1
			sample_project_index = -1
			try: 
				for line in csv_file:
					# It's the header for the sample list.
					if 'Sample_Name' in line and 'Sample_ID' in line and 'Sample_Project' in line: 
						sample_name_index = line.index('Sample_Name')
						sample_id_index = line.index('Sample_ID')
						sample_project_index = line.index('Sample_Project')
					# It's a line for which we might have some use.
					if sample_name_index != -1 and sample_id_index != -1 and sample_project_index != -1:
						if str(path) not in samplesheet_cache:
							samplesheet_cache[str(path)] = samplesheet_lines
					samplesheet_lines.append(line)
			# Usefully handle garbage text in the samplesheets rather than having a generic error/traceback.
			except UnicodeDecodeError as ude: 
				lineGuess = ude.object[:ude.start].count(b'\n') + 1 
				sys.exit("Found a bad char (non-UTF-8) in file " + str(path) + " at byte " + str(ude.start) + " around line " + str(lineGuess)) 

			if sample_name_index == -1:
				print("Missing Sample_Name column in Illumina samplesheet "+str(path)+", skipping")
			if sample_id_index == -1:
				print("Missing Sample_ID column in Illumina samplesheet "+str(path)+", skipping")
			if sample_project_index == -1:
				print("Missing Sample_Project column in Illumina samplesheet "+str(path)+", skipping")


def get_file(project, subject, sample_name, is_rna, suffix):
	#print("Looking for " + project + " " + sample_name + " " + suffix)
	# Code that returns a list of bam files for based on *sample_name* 
	if is_rna:
		return output_dir+'/{project}/{subject}/rna/'+sample_name+suffix
	else:
		return output_dir+'/{project}/{subject}/'+sample_name+suffix
	# If the file with the given suffix hasn't been generated yet, find which sequencing run has it.
	if not files:
		# Allows us to cache the samplesheet info.
		load_samplesheet_info()
		for path_str in samplesheet_cache:
			sample_name_index = -1
			sample_id_index = -1
			sample_project_index = -1
			for line in samplesheet_cache[path_str]:
				if 'Sample_Name' in line and 'Sample_ID' in line and 'Sample_Project' in line: # It's the header for the sample list.
					sample_name_index = line.index('Sample_Name')
					sample_id_index = line.index('Sample_ID')
					sample_project_index = line.index('Sample_Project')
				elif sample_name_index != -1 and sample_id_index != -1 and sample_project_index != -1 and line[sample_name_index] == sample_name:
					if not "RNA" in line[sample_project_index]:
						#print("Found sample "+sample_name+ " in samplesheet " + path_str)
						return analysis_dir+'/secondary/'+sequencer+'/'+os.path.splitext(os.path.basename(Path(path_str)))[0]+'/'+sample_name+suffix;
			
def get_normal_bam(wildcards):
	return get_file(wildcards.project, wildcards.subject, wildcards.normal, False, ".bam")

def get_tumor_bam(wildcards):
	return get_file(wildcards.project, wildcards.subject, wildcards.tumor, False, ".bam")

def get_tumor_rna_bam(wildcards):
	return get_file(wildcards.project, wildcards.subject, wildcards.tumor, True, ".bam")

# Some immunotherapy treatments are more or less suitable based on MHC class I (major A/B/C) HLA types, e.g. HLA-A*03 may be contraindicated for checkpoint inhibitor therapy per doi:10.1016/S1470-2045(21)00582-9
rule generate_hla_types:
	priority: 0
	resources:
		slurm_extra="--gres=fpga:1 --mem-per-cpu=0"
	input:
		normal_bam=get_normal_bam
	output:
		output_dir+'/{project}/{subject}/{subject}_{tumor}_{normal}.dna.somatic.hla.tsv'
	shell:
		"dragen --enable-hla=true --output-directory={output_dir}/{wildcards.project}/{wildcards.subject} --output-file-prefix={wildcards.subject}_{wildcards.tumor}_{wildcards.normal}.dna.somatic --ref-dir={refgenome} --bam-input={input.normal_bam}"

# Germline cancer susceptibility reporting
rule generate_cpsr_json:
	input:
		germline_snv_vcf = output_dir+'/{project}/{subject}/{subject}_{normal}.dna.germline.hard-filtered.vcf.gz'
	output:
		output_dir+'/{project}/{subject}/{normal}.cpsr.grch38.pass.tsv.gz'
	conda:
		"pcgr"
	shell:
		"cpsr --input_vcf {input.germline_snv_vcf} --vep_dir . --refdata_dir . --output_dir {output_dir}/{wildcards.project}/{wildcards.subject} --genome_assembly grch38 --panel_id 0 --sample_id {wildcards.normal} --secondary_findings --classify_all --maf_upper_threshold 0.2 --force_overwrite"

rule generate_minimal_maf:
	input:
		somatic_snv_vcf = output_dir+'/{project}/{subject}/{subject}_{tumor}_{normal}.dna.somatic.hard-filtered.vcf.gz'
	output:
		maf = output_dir+'/{project}/{subject}/{subject}_{tumor}_{normal}.dna.somatic.hard-filtered.maf.txt'
	run:
		# From the VCF generate the minimal MAF file that can be subsequently run through the Genome Nexus Annotation Pipeline 
		# (https://github.com/genome-nexus/genome-nexus-annotation-pipeline) to generate a 32 column TCGA MAF formatted file suitable
		# for upload to cBioPortal.
		# Minimal MAF (per https://docs.cbioportal.org/file-formats/#create-the-cbioportal-mutation-data-file-with-genome-nexus-with-a-minimal-maf-file): 
		# Chromosome (Required): A chromosome number, e.g., "7".
		# Start_Position (Required): Start position of event.
		# End_Position (Required): End position of event.
		# Reference_Allele (Required): The plus strand reference allele at this position.
		# Tumor_Seq_Allele2 (Required): Primary data genotype.
		# Tumor_Sample_Barcode (Required): This is the sample ID. Either a TCGA barcode (patient identifier will be extracted), or for non-TCGA data, a literal SAMPLE_ID as listed in the clinical data file.
		# In addition to the above columns, it is recommended to have the read counts to calculate variant allele frequencies:
		# t_alt_count (Optional, but recommended): Variant allele count (tumor).
		# t_ref_count (Optional, but recommended): Reference allele count (tumor).
		shell("gzip -cd {input.somatic_snv_vcf} | perl -F\\t -ane '$F[0] =~ s/^chr//; print join(\"\\t\", $F[0], $F[1], $F[1]+length($F[3])-1, $F[3], $F[4], $2, $1, \"{wildcards.tumor}\"), \"\\n\" if not /^#/ and $F[$#F] =~ /^[^:]+:[^:]+:(\d+),(\d+)/' > {output.maf}")

# Personal Cancer Genome Report (user-friendly triaged variants self-contained Web page)
# The Web page name is limited by PCGR to 35 characters, so we have the full subject-tumor-normal unique combo in the dir name only.
rule generate_pcgr_html:
	input:
		somatic_snv_vcf = output_dir+'/{project}/{subject}/{subject}_{tumor}_{normal}.dna.somatic.hard-filtered.vcf.gz',
		somatic_cnv_vcf = output_dir+'/{project}/{subject}/{subject}_{tumor}_{normal}.dna.somatic.cnv.vcf.gz'
	output:
		output_dir+'/pcgr/{project}/{subject}_{tumor}_{normal}/{subject}.pcgr_acmg.grch38.html'
	conda:
		"pcgr"
	shell:
		# Wrapper script to reformat inputs and run PCGR, so we can use PCGR's conda env directly.
		"./generate_pcgr.py {input.somatic_snv_vcf} {input.somatic_cnv_vcf} " +
		# DNA sample spec
		"{output_dir} {wildcards.project} {wildcards.subject} {wildcards.tumor} {wildcards.normal}" 

# Used by PCGR for reporting known germline cancer susceptibility or related variants.
# The germline ("Normal") BAM file was generated by the somatic variant Dragen call.
rule dragen_germline_snv_vcf:
	priority: 100
	resources:
		slurm_extra="--gres=fpga:1 --mem-per-cpu=0"
	input:
		germline_bam = get_normal_bam
	output:
		output_dir+'/{project}/{subject}/{subject}_{normal}.dna.germline.hard-filtered.vcf.gz'
	shell:
		"dragen -b {input.germline_bam} -r {refgenome} --output-directory {output_dir}/{wildcards.project}/{wildcards.subject} --output-file-prefix {wildcards.subject}_{wildcards.normal}.dna.germline --enable-variant-caller true --enable-cnv true --intermediate-results-dir {temp_dir} --enable-map-align false"

rule dragen_bcl_conversion:
	# If the run was generated using a library prep kit that includes UMIs, this must be noted in the samplesheet to allow proper grouping of the reads.
	# See config.yaml for more details.
	input:
		csv=get_samplesheet
	output:
		analysis_dir+'/primary/{sequencer}/{run}/Reports/fastq_list.csv'
	shell:
		"bcl-convert --force --sample-sheet {input.csv} --bcl-input-directory {bcl_dir}/{sequencer}/{wildcards.run} --output-directory {analysis_dir}/primary/{sequencer}/{wildcards.run}"

# One or more libraries correspond to a single sample defined by the wildcard match values for a sequencing run. 
# Generate a file listing all those FASTQ files so they can be processed together, e.g. for reference mapping.
def make_sample_fastq_list_csv(wildcards, is_rna, sample_libraries):
	# We could generate a combined FASTQ file, but this takes up a lot of extra space and tmp dir might fill for somatic genomes for example
	# Instead we will generate a fastq-list file that contains all the libraries for the sample (you may have more than one as in XP loading you can't name same sample ID across lanes for same sample name) 
	# Since a sample could have been spread across multiple runs, we need to have a run-agnostic name for the FASTQ list file. The natural spot is the "output" dir, even if
	# this is not an output in any practical sense of the pipeline.
	if is_rna:
		sample_fastq_list_csv = output_dir+'/'+wildcards.project+'/'+wildcards.subject+'/rna/'+wildcards.sample+'_fastq_list.csv'
	else:
		sample_fastq_list_csv = output_dir+'/'+wildcards.project+'/'+wildcards.subject+'/'+wildcards.sample+'_fastq_list.csv'
	header_printed = False
	with open(sample_fastq_list_csv, 'w') as f:
		# Gather file specs from all run that have been converted.
		for fastq_list_csv in glob.iglob(analysis_dir+'/primary/'+sequencer+'/*/Reports/fastq_list.csv'):
			#print('Reading samples from '+fastq_list_csv+' while looking for sample ' + wildcards.sample)
			with open(fastq_list_csv, 'r') as data_in:
				csv_file = csv.reader(data_in)
				if not header_printed:
					# Print the header line
					f.write(",".join(next(csv_file)))
					f.write('\n')
					header_printed = True
				for line in csv_file:
					if line[1] in sample_libraries:
						# We might have different sample names for the same sample like Li###-lane1
						# and this will cause problems downstream, e.g. when somatic mutation calling
						# and dragen enforces a single sample name in the source BAM. So replace any existing name.
						# Otherwise post hoc you'll need to do something like 
						# samtools reheader -c 'perl -ne "s/^(\@RG.*\s+SM:\S+)-lane\d/$1/"'
						line[2] = wildcards.sample
						# We may also have rewritten the .fastq.gz as a .ora file to save room. In that casea we need to write the .ora name to the new list
						if not os.path.isfile(line[4]):
							orafile = line[4].replace(".fastq.gz",".fastq.ora")
							if not os.path.isfile(orafile):
								print('Cannot find target FASTQ file specified in ' + fastq_list_csv  + ', may fail subsequently without: ' + line[4])
							line[4] = orafile
						if not os.path.isfile(line[5]):
							orafile = line[5].replace(".fastq.gz",".fastq.ora")
							if not os.path.isfile(orafile):
								print('Cannot find target FASTQ file specified in ' + fastq_list_csv  + ', may fail subsequently without: ' + line[5])
							line[5] = orafile
						f.write(",".join(line))
						f.write('\n')
	return sample_fastq_list_csv

def get_sample_fastq_list_csvs(wildcards, is_rna):
	library_info = identify_libraries(is_rna, wildcards)
	all_csvs_with_sample = [];
	# Gather file specs from all run that have been converted.
	for fastq_list_csv in glob.iglob(analysis_dir+'/primary/'+sequencer+'/*/Reports/fastq_list.csv'):
		with open(fastq_list_csv, 'r') as data_in:
			csv_file = csv.reader(data_in)
			for line in csv_file:
				if line[1] in library_info[1]:
					all_csvs_with_sample.append(fastq_list_csv)
					break;
	return all_csvs_with_sample

def get_dna_sample_fastq_list_csvs(wildcards):
	return get_sample_fastq_list_csvs(wildcards, False)

def get_rna_sample_fastq_list_csvs(wildcards):
	return get_sample_fastq_list_csvs(wildcards, True)

# Return a tuple of two:
# Boolean as to whether this library should get the Unique Molecular Indices treatment, using info from the SampleSheet.csv
# The list of library names (that will be in the FASTQ list CSV) that correspond to the sample name identified by the wildcards 
def identify_libraries(is_rna, wildcards):
	# Figure out the library ID (which is in the FASTQ file names) from the sample name in the samplesheet
	# If XP loading was used, you can have multiple "libraries" called Li###-lane1, etc. for the same sample on a run as IEV requires unique sample IDs and normally we'd use the library ID
	sample_has_UMIs = False
	sample_libraries = []
	for samplesheet in glob.iglob(samplesheet_dir+'/*.csv'):
		run_has_UMIs = False
		with open(samplesheet, 'r') as data_in:
			sample_project_index = -1
			sample_name_index = -1
			sample_id_index = -1
			#print("Gathering FASTQ file for "+wildcards.sample+" from "+samplesheet)
			csv_file = csv.reader(data_in)
			for line in csv_file:
				if "Sample_Name" in line and "Sample_ID" in line and "Sample_Project" in line: #it's the header for the sample list
					sample_name_index = line.index("Sample_Name")
					sample_id_index = line.index("Sample_ID")
					sample_project_index = line.index("Sample_Project")
				elif sample_name_index != -1 and sample_id_index != -1 and line[sample_name_index] == wildcards.sample:
					if is_rna:
						if "RNA" in line[sample_project_index] and not "test" in line[sample_project_index]:
							sample_libraries.append(line[sample_id_index])
							sample_has_UMIs = run_has_UMIs
					else:
						if not "RNA" in line[sample_project_index]:
							sample_libraries.append(line[sample_id_index]) 
							sample_has_UMIs = run_has_UMIs
				elif len(line) > 1 and line[0] == "OverrideCycles" and "U" in line[1]:
					run_has_UMIs = True
			# Uncomment below if you want to be pedantic.
			#if sample_name_index == -1:
			#	raise NameError("Missing Sample_Name column in Illumina samplesheet "+samplesheet)
			#if sample_id_index == -1:
			#	raise NameError("Missing Sample_ID column in Illumina samplesheet "+samplesheet)
	return sample_has_UMIs, list(set(sample_libraries)) # dedup

rule dragen_dna_read_mapping:
	# This rule will check if the library was UMI-indexed or not, and change the mapping procedure accordingly.
	priority: 100
	resources:
		slurm_extra="--gres=fpga:1 --mem-per-cpu=0"
	input:
		get_dna_sample_fastq_list_csvs,
		# Omitting dependency on index date because Dragen 3.10 to 4.2 caused index changes that would require rerunning all old samples. 
		#refgenome+'/reference.bin'
	output:
		# Avoid greedy default wildcard that'll suck up "project/sample" prefix into the project wildcard and give us "rna" as a sample name if that subdir exists with the RNA BAM file in it.
		output_dir+'/{project,[^/]+}/{subject,[^/]+}/{sample,[^/]+}.bam'
	run:
		library_info = identify_libraries(False, wildcards)
		has_UMIs = library_info[0]
		sample_libraries = library_info[1]
		print("Using UMIs: " + str(has_UMIs))
		print("Libraries: " + ", ".join(sample_libraries))
		this_sample_only_fastq_list_csv = make_sample_fastq_list_csv(wildcards, False, sample_libraries)
		if has_UMIs:
			shell("dragen -r {refgenome} --ora-reference {ora_reference} --output-dir {output_dir}/{wildcards.project}/{wildcards.subject} --output-file-prefix {wildcards.sample} --enable-map-align true --enable-sort true --enable-map-align-output true --enable-bam-indexing true --umi-enable true --umi-correction-scheme=random --umi-min-supporting-reads 1 --umi-min-map-quality 1 --fastq-list {this_sample_only_fastq_list_csv} --fastq-list-all-samples true --intermediate-results-dir {temp_dir}")
		else:
			shell("dragen -r {refgenome} --ora-reference {ora_reference} --output-dir {output_dir}/{wildcards.project}/{wildcards.subject} --output-file-prefix {wildcards.sample} --enable-map-align true --enable-sort true --enable-map-align-output true --enable-bam-indexing true --fastq-list {this_sample_only_fastq_list_csv} --fastq-list-all-samples true --intermediate-results-dir {temp_dir}")

rule dragen_rna_read_mapping_quant_and_fusion_calls:
	priority: 102
	resources:
		slurm_extra="--gres=fpga:1 --mem-per-cpu=0"
	input:
		get_rna_sample_fastq_list_csvs,
		refgenome+'/anchored_rna',
		ref_exon_annotations
	output:
		#bam = "{output_dir}/{project}/{subject}/rna/{subject}_{sample}.rna.bam",
		sf = "{output_dir}/{project}/{subject}/rna/{subject}_{sample}.rna.quant.genes.sf",
		csv = "{output_dir}/{project}/{subject}/rna/{subject}_{sample}.rna.fusion_candidates.features.csv"
	run:
		library_info = identify_libraries(True, wildcards)
		sample_libraries = library_info[1]
		this_sample_only_fastq_list_csv = make_sample_fastq_list_csv(wildcards, True, sample_libraries)
		shell("dragen -r {ref_anchored_genome} --ora-reference {ora_reference} --fastq-list {this_sample_only_fastq_list_csv} --fastq-list-all-samples true --intermediate-results-dir {temp_dir} --output-dir {output_dir}/{wildcards.project}/{wildcards.subject}/rna --output-file-prefix {wildcards.subject}_{wildcards.sample}.rna --enable-sort true --enable-rna true --enable-map-align true --enable-map-align-output true --enable-bam-indexing true --enable-rna-gene-fusion true --enable-rna-quantification true --annotation-file {ref_exon_annotations} --force")
		# If there are no fusion gene candidtes, no output files are created. In this case, created a blank one so we don't rerun this analysis or fail out due to lack of output file.
		if not Path(output.csv).is_file():
			shell("touch {output.csv}")

rule annotate_rna_genes:
	priority: 101
	input:
		annotations = ref_exon_annotations,
		sf = "{output_dir}/{project}/{subject}/rna/{subject}_{tumor}.rna.quant.genes.sf"
	output:
		hugo_tpm_file = "{output_dir}/{project}/{subject}/rna/{subject}_{tumor}.rna.quant.genes.hugo.tpm.txt"

	shell:
		# Generate Transcripts Per Million normalized data for the RNA with HUGO gene names, this can be used downstream by packages like immunedeconv.
		# In the GTF annotations, we might have mutliple gene IDs mappinga to the same HUGO (e.g, this is true in the Gencode GTF), so sum values with same HUGO mapping.
		"perl -F\\\\t -ane \'BEGIN{{print \"Hugo_Symbol\\t{wildcards.tumor}\\n\"; for(split /\\n/s, `grep gene_name {input.annotations}`){{$gene2hugo{{$1}} = $2 if /gene_id \"(\\S+)\".*gene_name \"(\\S+)\"/}}}}" +
                      "$hugo_sum{{$gene2hugo{{$F[0]}}}} += $F[3] if $. > 1; END{{for(sort keys %hugo_sum){{print $_,\"\\t\",$hugo_sum{{$_}},\"\\n\" }}}}\' {input.sf} > {output.hugo_tpm_file}"

rule dragen_exec_somatic_snv_sv_and_cnv_calls:
	priority: 98
	resources:
		slurm_extra="--gres=fpga:1 --mem-per-cpu=0"
	input:
		tumor_bam = get_tumor_bam,
		germline_bam = get_normal_bam
	output:
		"{output_dir}/{project}/{subject}/{subject}_{tumor}_{normal}.dna.somatic.cnv.vcf.gz",
		"{output_dir}/{project}/{subject}/{subject}_{tumor}_{normal}.dna.somatic.hard-filtered.vcf.gz",
		"{output_dir}/{project}/{subject}/{subject}_{tumor}_{normal}.dna.somatic.sv.vcf.gz"
	run:
		dragen_cmd = "dragen --enable-map-align false --bam-input {input.germline_bam} --tumor-bam-input {input.tumor_bam} -r {refgenome} --output-directory {output_dir}/{wildcards.project}/{wildcards.subject} --output-file-prefix {wildcards.subject}_{wildcards.tumor}_{wildcards.normal}.dna.somatic --enable-cnv true --intermediate-results-dir {temp_dir} --enable-variant-caller true --vc-enable-unequal-ntd-errors=true --vc-enable-trimer-context=true --enable-sv true --cnv-use-somatic-vc-baf true -f"
		has_tumor_in_normal = get_normal_contains_some_tumor(wildcards)
		if has_tumor_in_normal:
			shell(dragen_cmd +  " --sv-enable-liquid-tumor-mode true --sv-tin-contam-tolerance {tumor_in_normal_tolerance_proportion}")
		else:
			shell(dragen_cmd)
		shell("mv {output_dir}/{wildcards.project}/{wildcards.subject}/sv/results/variants/somaticSV.vcf.gz {output_dir}/{wildcards.project}/{wildcards.subject}/{wildcards.subject}_{wildcards.tumor}_{wildcards.normal}.dna.somatic.sv.vcf.gz; mv {output_dir}/{wildcards.project}/{wildcards.subject}/sv/results/variants/somaticSV.vcf.gz.tbi {output_dir}/{wildcards.project}/{wildcards.subject}/{wildcards.subject}_{wildcards.tumor}_{wildcards.normal}.dna.somatic.sv.vcf.gz.tbi")
